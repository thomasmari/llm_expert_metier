# -*- coding: utf8 -*-
#
# Auteur : Xavier BEDNAREK
# Date : 10/09/2025

from mytools import setup_env_variables, create_file_if_not_exists
from langchain_chroma import Chroma
import datetime
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_google_genai import GoogleGenerativeAI
from langchain_core.documents import Document

class AIExpertLawyer() : 
    """Classe d√©finissant un Agent IA expert en droit penal
    """
    
    # --------------------------------------------------------------------------
    #                                                               Constructeur
    # --------------------------------------------------------------------------

    def __init__(self, *, system_prompt: str| None = None, chroma_collection_name: str = "code_penal", chroma_db_path : str = "./chroma_langchain_db", llm_model : str = "gemini-2.5-flash-lite", temperature : float = 0.3, top_p: float = 0.8, nb_chunk : int = 4, logfile : str|None = "logs/log_AIExpertLawyer.txt") -> None:
        """Constructeur de l'Agent IA"""

        # Setup des variables d'environnement
        setup_env_variables(auto=True, verbose=False)

        # 1 - Param√©trage de la base de donn√©e s√©mantique (Chroma)

        embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

        self._vector_store = Chroma(
            collection_name=chroma_collection_name,
            embedding_function=embeddings,
            persist_directory=chroma_db_path,  # Where to save data locally, remove if not necessary
        )

        # 2 - Param√©trage du LLM
        self._llm = GoogleGenerativeAI(model=llm_model, 
                                       temperature=temperature,  # Entre 0.0 et 1.0
                                       top_p=top_p               # Entre 0.0 et 1.0
                                       )

        # 3 - Meta prompt utilis√© :
        if not system_prompt : # On fait un prompt par d√©faut
            self._system_prompt = ("<system prompt>:Tu es un juriste expert et tu r√©ponds √† des questions juridiques. Ton but est de r√©pondre au <user prompt>, et tu peux utiliser les donn√©es <rag data> issues d'une recherche de similarit√© dans une base de donn√©es vectoris√©e du code p√©nal.\n\n" +
                                   "<rag data>:\n{rag_data}\n\n"+
                                   "<user prompt>:\n{user_prompt}\n")
        else :
            self._system_prompt = system_prompt

        # 4 - Param√©trage de la fa√ßon dont sont faites les requ√™tes dans la base de donn√©e s√©mantique
        self._nb_chunks = nb_chunk

        # 5 - Gestion des logs
        self._logfile = logfile
        if self._logfile  is not None :
            create_file_if_not_exists(self._logfile)
        self.log("="*80 + "\n ! ! CREATING NEW AIExpertLawyer ! ! ")

    # --------------------------------------------------------------------------
    #                                                                   M√©thodes
    # --------------------------------------------------------------------------

    def __str__(self) -> str:
        return (
         "==========================================\n"   
         "AIExpertLawyer with following parameters :\n" +
        f"   - system_prompt :\n     ---------------\n     {self._system_prompt.strip().replace("\n","\n     ")}\n     ---------------\n" +
        f"   - llm :\n     {str(self._llm).replace("\n","\n     ")}\n" + 
        f"   - nb_chunks : {self._nb_chunks}\n" + 
        "=========================================="   
        )
    
    def log(self, text:str):
        """Log quelque chose dans le log file (pour le suivit des requ√™tes par ex)"""
        if self._logfile is not None :
            with open(self._logfile, 'a', encoding='utf-8') as f:
                f.write(("-"*80)+"\n> Log (" + datetime.datetime.now().strftime("%A, %d. %B %Y %H:%M:%S") + ") :\n" +text+"\n")
    
    def get_system_prompt(self) -> str :
        return self._system_prompt
    
    def ask(self, question:str) -> str:
        """Demande quelque chose √† notre agent"""

        # 1 - Requ√™te dans la base de donn√©e s√©mantique :
        similarity_results = self.request_in_semantic_db(question)

        # 2 - Cr√©ation du prompt √† envoyer au LLM
        prompt = self._system_prompt.format(rag_data=similarity_results, user_prompt=question)
        self.log("On interroge le LLM de l'expert avec le prompt :\n"+prompt)

        # 3 - Appelle du LLM
        reponse = self._llm.invoke(prompt)
        self.log("La r√©ponse du LLM est :\n"+reponse)
        return reponse

    def request_in_semantic_db(self, query:str) -> list[Document] :
        """Fait une requ√™te dans la base de donn√©e s√©mantique"""
        return self._vector_store.similarity_search(query=query, k=self._nb_chunks)


if __name__=='__main__':

    # Test

    # Cr√©ation de l'agent
    expert = AIExpertLawyer(temperature=0.25, nb_chunk=3, top_p=0.5, logfile=None)
    print("üîé  Info sur l'Agent utilis√© :")
    print(expert)

    # Question :
    question = "Quel est la peine de prison la plus longue envisageable ?"
    print("### Question :")
    print(f"> {question}", flush=True)

    # R√©ponse :
    reponse = expert.ask(question)
    print("###  R√©ponse :")
    print(f"> {reponse.replace("\n", "\n> ")}")
