{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714a926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement google-generative-ai (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for google-generative-ai\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install google-generative-ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4789904e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.generativeai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenai\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Configure Gemini API\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.generativeai'"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=userdata.get('GENAI_API'))\n",
    "\n",
    "def rag_query(query_text, n_results=3, max_chars_per_doc=1000):\n",
    "    \"\"\"\n",
    "    Fonction RAG : r√©cup√®re les CVs via Chroma et g√©n√®re une r√©ponse avec Gemini.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): La question ou requ√™te utilisateur\n",
    "        n_results (int): Nombre de documents CV √† r√©cup√©rer\n",
    "        max_chars_per_doc (int): Nombre max de caract√®res √† prendre par document pour le contexte\n",
    "    Returns:\n",
    "        str: R√©ponse g√©n√©r√©e par le LLM bas√©e sur les CVs\n",
    "    \"\"\"\n",
    "\n",
    "    # √âtape 1 : r√©cup√©rer les documents via Chroma\n",
    "    results = simple_query(query_text, n_results=n_results)\n",
    "    \n",
    "    if not results or not results.get(\"documents\") or not results[\"documents\"][0]:\n",
    "        return \"‚ùå Aucun CV pertinent trouv√© pour cette requ√™te.\"\n",
    "\n",
    "    # √âtape 2 : construire le contexte √† partir des extraits\n",
    "    documents = results[\"documents\"][0]\n",
    "    metadatas = results[\"metadatas\"][0]\n",
    "    \n",
    "    context_chunks = []\n",
    "    for doc, meta in zip(documents, metadatas):\n",
    "        preview = doc[:max_chars_per_doc] + (\"...\" if len(doc) > max_chars_per_doc else \"\")\n",
    "        meta_info = f\"üë§ Candidate: {meta.get('candidate_name', 'Unknown')}, \" \\\n",
    "                    f\"üìç Location: {meta.get('location', 'N/A')}, \" \\\n",
    "                    f\"üíº Seniority: {meta.get('seniority_level', 'N/A')}\"\n",
    "        context_chunks.append(f\"{meta_info}\\n{preview}\")\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "\n",
    "    # √âtape 3 : construire le prompt pour le LLM\n",
    "    prompt = f\"\"\"\n",
    "Vous √™tes un assistant RH. Voici des extraits pertinents de CVs correspondant √† la requ√™te :\n",
    "{context}\n",
    "\n",
    "R√©pondez de mani√®re concise et informative √† la question suivante, en vous basant uniquement sur ces CVs :\n",
    "{query_text}\n",
    "\"\"\"\n",
    "    \n",
    "    # √âtape 4 : g√©n√©ration via Gemini\n",
    "    response = genai.chat.create(\n",
    "        model=\"models/gemini-1.5-chat\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    return response.last.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ccf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "#query = \"Cherche un d√©veloppeur Python avec exp√©rience en machine learning et data science.\"\n",
    "query = \"les punitions le plus important dans le code p√©nal.\"\n",
    "answer = rag_query(query)\n",
    "print(\"üí° R√©ponse RAG :\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f83f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400404c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-expert-metier (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
